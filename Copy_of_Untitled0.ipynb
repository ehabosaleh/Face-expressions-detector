{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNAr+Fz0WVv70GhFw7o0ucJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehabosaleh/Face-expressions-detector/blob/main/Copy_of_Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsQiii8wlkzj",
        "outputId": "fda0975e-b78d-4c9b-f3bb-6a8b0d275a08"
      },
      "source": [
        "#import the DataSet\n",
        "!git clone https://github.com/misbah4064/facial_expressions.git\n",
        "#   تحميل قاعدة بيانات تمثل نماذج عن التعابير الوجهية"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'facial_expressions'...\n",
            "remote: Enumerating objects: 14243, done.\u001b[K\n",
            "remote: Total 14243 (delta 0), reused 0 (delta 0), pack-reused 14243\u001b[K\n",
            "Receiving objects: 100% (14243/14243), 240.06 MiB | 42.95 MiB/s, done.\n",
            "Resolving deltas: 100% (232/232), done.\n",
            "Checking out files: 100% (14004/14004), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVIqs0SfxyDg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr5emhUvg-rw",
        "outputId": "3fbcd591-db51-4abf-a748-e596b58fa6c5"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "anger.txt   facial_expressions\t\t\t neutral.txt  surprise.txt\n",
            "data\t    haarcascade_frontalface_default.xml  python       test\n",
            "data_set    happy.txt\t\t\t\t R\n",
            "dwayne.jpg  images\t\t\t\t README.md\n",
            "elon.jpg    LICENSE\t\t\t\t sad.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCmVri8vwIXm",
        "outputId": "aebd5d4f-abd6-41d3-ee53-fa874cbfba0d"
      },
      "source": [
        "!pip3 install opencv-python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_DM4U4QltIj",
        "outputId": "90aca286-4b96-46c7-cee1-27bc9042e29d"
      },
      "source": [
        "#Creating All Necessary Directories\n",
        "%cd facial_expressions/\n",
        "%mkdir --parents  data_set/{anger,happy,neutral,sad,surprise}\n",
        "#نشاء مجلدات يحتوي كل واحد على تعبير وجه محدد"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/facial_expressions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcnkjrY9lxgV",
        "outputId": "cb7bbcd9-059e-48fc-8cb7-c8042fa4da08"
      },
      "source": [
        "#Extracting Images With Expressions\n",
        "import  cv2\n",
        "with  open('happy.txt','r')  as  f:\n",
        "  img = [line.strip() for line in f]\n",
        "for image in img:\n",
        "  loadedImage = cv2.imread(\"images/\"+image)\n",
        "  cv2.imwrite(\"data_set/happy/\"+image,loadedImage)\n",
        "print(\"done writing\")\n",
        "#   باللاستفادة من المكتبةcv2 قراءة الصور التي تمثل تعبير الفرح "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done writing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SOzDwZPxEUV"
      },
      "source": [
        "%mkdir dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1wDM9okl5Ch",
        "outputId": "c6f29255-b96b-44e2-b1b1-14addefe0251"
      },
      "source": [
        "#First we create Data Set of Faces\n",
        "import cv2\n",
        "\n",
        "with open('happy.txt','r') as f:\n",
        "  images = [line.strip() for line in f]\n",
        "\n",
        "face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "# For each Emotion, enter one numeric face id\n",
        "face_id = input('\\n Enter Emotion id end press <return> ==>  ')\n",
        "\n",
        "count = 0\n",
        "\n",
        "for image in images:\n",
        "  img = cv2.imread(\"data_set/happy/\"+image)\n",
        "  gray  = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  faces = face_detector.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "  for (x,y,w,h) in faces:\n",
        "        cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0), 2)\n",
        "        count += 1\n",
        "        # Save the captured image into the datasets folder\n",
        "        cv2.imwrite(\"dataset/User.\" + str(face_id) + '.' + str(count) + \".jpg\", gray[y:y+h,x:x+w])\n",
        "\n",
        "print(\"\\n Done creating face data\")\n",
        "# اكتشاف نماذج الوجوه من كل صورة في قاعدة البيانات و حفظها"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Enter Emotion id end press <return> ==>  1\n",
            "\n",
            " Done creating face data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkSUIcES9TD9",
        "outputId": "410b874e-d04b-40d1-c395-dc2b9d8d36b0"
      },
      "source": [
        "%mkdir trainer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘trainer’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUlb8_5XoJvO",
        "outputId": "3442cadf-3b94-4d56-9f76-677d939894f9"
      },
      "source": [
        "#Second we train all images\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Path for face image database\n",
        "path = 'dataset'\n",
        "\n",
        "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
        "detector = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\");\n",
        "\n",
        "# function to get the images and label data\n",
        "def getImagesAndLabels(path):\n",
        "    imagePaths = [os.path.join(path,f) for f in os.listdir(path)]\n",
        "    faceSamples=[]\n",
        "    ids = []\n",
        "\n",
        "    for imagePath in imagePaths:\n",
        "        PIL_img = Image.open(imagePath).convert('L') # convert it to grayscale\n",
        "        img_numpy = np.array(PIL_img,'uint8')\n",
        "\n",
        "        id = int(os.path.split(imagePath)[-1].split(\".\")[1])\n",
        "        faces = detector.detectMultiScale(img_numpy)\n",
        "\n",
        "        for (x,y,w,h) in faces:\n",
        "            faceSamples.append(img_numpy[y:y+h,x:x+w])\n",
        "            ids.append(id)\n",
        "    return  faceSamples,ids\n",
        "\n",
        "print (\"\\n [INFO] Training faces....\")\n",
        "faces,ids = getImagesAndLabels(path)\n",
        "recognizer.train(faces, np.array(ids))\n",
        "\n",
        "# Save the model into trainer/trainer.yml\n",
        "recognizer.write('trainer/trainer.yml')\n",
        "\n",
        "# Print the numer of Emotions trained and end program\n",
        "print(\"\\n [INFO] {0} Emotions trained. Exiting Program\".format(len(np.unique(ids))))\n",
        "#تدريب المصنف باستخدام النماذج التي تم استخلاصها من الكود السابق"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " [INFO] Training faces....\n",
            "\n",
            " [INFO] 1 Emotions trained. Exiting Program\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "l_hNnfs4oRNZ",
        "outputId": "7d074f54-ce37-4990-a27c-467ea7756416"
      },
      "source": [
        "# Now we do the testing or Recognition\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
        "recognizer.read('trainer/trainer.yml')\n",
        "cascadePath = \"haarcascade_frontalface_default.xml\"\n",
        "faceCascade = cv2.CascadeClassifier(cascadePath);\n",
        "\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "\n",
        "#iniciate id counter\n",
        "id = 0\n",
        "\n",
        "# Emotions related to ids: example ==> Anger: id=0,  etc\n",
        "names = ['Anger', 'Happy', 'None', 'None', 'None', 'None']\n",
        "\n",
        "# Initialize and start realtime video capture\n",
        "cam = cv2.VideoCapture(0)\n",
        "cam.set(3, 640) # set video widht\n",
        "cam.set(4, 480) # set video height\n",
        "\n",
        "# Define min window size to be recognized as a face\n",
        "minW = 0.1*cam.get(3)\n",
        "minH = 0.1*cam.get(4)\n",
        "\n",
        "# ret, img =cam.read()\n",
        "img = cv2.imread(\"elon.jpg\")\n",
        "# img = cv2.flip(img, -1) # Flip vertically\n",
        "\n",
        "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "faces = faceCascade.detectMultiScale(gray,scaleFactor = 1.2, minNeighbors = 5, minSize = (int(minW), int(minH)),)\n",
        "\n",
        "for(x,y,w,h) in faces:\n",
        "\n",
        "    cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)\n",
        "\n",
        "    id, confidence = recognizer.predict(gray[y:y+h,x:x+w])\n",
        "\n",
        "    # Check if confidence is less them 100 ==> \"0\" is perfect match \n",
        "    if (confidence < 100):\n",
        "        id = names[id]\n",
        "        confidence = \"  {0}%\".format(round(100 - confidence))\n",
        "    else:\n",
        "        id = \"unknown\"\n",
        "        confidence = \"  {0}%\".format(round(100 - confidence))\n",
        "    \n",
        "    cv2.putText(img, str(id), (x+5,y-5), font, 1, (255,255,255), 2)\n",
        "    cv2.putText(img, str(confidence), (x+5,y+h-5), font, 1, (255,255,0), 1)  \n",
        "\n",
        "cv2.imwrite(\"elonTest.jpg\",img) \n",
        "\n",
        "print(\"\\n [INFO] Done detecting and Image is saved\")\n",
        "cam.release()\n",
        "cv2.destroyAllWindows()\n",
        "#اختبار المصنف بعد أن قمنا ببناءه في الكود السابق"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-7cbb199028f7>\"\u001b[0;36m, line \u001b[0;32m44\u001b[0m\n\u001b[0;31m    id = names[id]\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "emVwnzXaoXSi",
        "outputId": "0691c569-cd93-4625-cea6-4e28f03aba90"
      },
      "source": [
        "#Also we can display the outpot\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "image = cv2.imread(\"elonTest.jpg\")\n",
        "height, width = image.shape[:2]\n",
        "resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(18, 10)\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "#اظهار نموذج كمثال من اختبار المصنف"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-af0dfd2f677c>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    import cv2\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYYv-8pjzgxJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}